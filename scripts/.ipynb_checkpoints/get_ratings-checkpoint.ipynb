{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bs4 as bs\n",
    "import requests\n",
    "\n",
    "# get the ratings while preserving the link to the reviews\n",
    "url = 'http://www.theramenrater.com/resources-2/the-list/'\n",
    "r = requests.get(url)\n",
    "sp = bs.BeautifulSoup(r.content, 'lxml')\n",
    "tb = sp.find_all('table')[0] \n",
    "df = pd.read_html(str(tb),encoding='utf-8', attrs = {'id': 'myTable'}, header=0)[0]\n",
    "df['href'] = [tag.get('href') for tag in tb.find_all('a')]\n",
    "df.set_index('Review #', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function to find visible text in webpage\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, bs.Comment):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from numba import jit\n",
    "\n",
    "\n",
    "def remove_features(word, tagger, lmtzr):\n",
    "    \"\"\"Returns a word after it has been checked to see if it is worth keeping\"\"\"\n",
    "    nltk.data.path.append(\"/opt/gmi/bd_userapps/shared/nltk_data\")\n",
    "    function_list = [remove_stop_words, remove_puncuation, remove_numbers, filter_tag_pos, lemmatize_word,\n",
    "                     remove_short_words]\n",
    "    # lowercase\n",
    "    word = word.lower()\n",
    "    # iterate through functions and stop if the word gets thrown out\n",
    "    for func in function_list:\n",
    "        if func == filter_tag_pos:\n",
    "            word, tagged_text = func(word, tagger)\n",
    "        elif func == lemmatize_word:\n",
    "            word = func(tagged_text, lmtzr)\n",
    "        else:\n",
    "            word = func(word)\n",
    "        if word.isspace() or word == '':\n",
    "            break\n",
    "    return word\n",
    "\n",
    "\n",
    "@jit\n",
    "def filter_tag_pos(word, tagger):\n",
    "    \"\"\"Tag Part of Speach keep only verbs, nouns and adjectives\"\"\"\n",
    "    # noun tags\n",
    "    nn_tags = ['NN', 'NNP', 'NNP', 'NNPS', 'NNS']\n",
    "    # adjectives\n",
    "    jj_tags = ['JJ', 'JJR', 'JJS']\n",
    "    # verbs\n",
    "    vb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "    nltk_tags = nn_tags + jj_tags + vb_tags\n",
    "    tagged_text = tagger.tag([word])\n",
    "    # word & tag tuple\n",
    "    if tagged_text[0][1] not in nltk_tags:\n",
    "        word = ''\n",
    "    return word, tagged_text\n",
    "\n",
    "\n",
    "@jit\n",
    "def lemmatize_word(tagged_text, lmtzr):\n",
    "    if tagged_text[0][1][0].lower() == 'v':\n",
    "        word = lmtzr.lemmatize(tagged_text[0][0], pos='v')\n",
    "    elif tagged_text[0][1][0].lower() == 'n':\n",
    "        word = lmtzr.lemmatize(tagged_text[0][0], pos='n')\n",
    "    else:\n",
    "        word = tagged_text[0][0]\n",
    "    return word\n",
    "\n",
    "\n",
    "@jit\n",
    "def remove_short_words(word):\n",
    "    if len(word) < 3:\n",
    "        word = ''\n",
    "    return word\n",
    "\n",
    "\n",
    "@jit\n",
    "def remove_stop_words(word):\n",
    "    \"\"\"take a word and check it against the common stop words list from NLTK\"\"\"\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    if word in stops:\n",
    "        word = ''\n",
    "    return word\n",
    "\n",
    "\n",
    "@jit\n",
    "def remove_puncuation(word):\n",
    "    # compile regex\n",
    "    punc_re = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation\n",
    "    word = punc_re.sub('', word)\n",
    "    return word\n",
    "\n",
    "\n",
    "@jit\n",
    "def remove_numbers(word):\n",
    "    # compile regex\n",
    "    num_re = re.compile('(\\\\d+)')\n",
    "    # remove numbers\n",
    "    word = num_re.sub('', word)\n",
    "    return word\n",
    "\n",
    "\n",
    "def text_cleaner(raw_str, tagger, lmtzr):\n",
    "    \"\"\"Returns a cleaned row after removing words not needed\"\"\"\n",
    "    clean_words = []\n",
    "    for word in word_tokenize(raw_str):\n",
    "        clean_words.append(remove_features(word, tagger, lmtzr))\n",
    "    clean_str = \" \".join(map(str, clean_words))\n",
    "    # remove redundant spaces\n",
    "    clean_str = re.sub('\\s\\s+', ' ', clean_str)\n",
    "    return clean_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# init the nltk objects once\n",
    "lmtzr = WordNetLemmatizer()\n",
    "tagger = PerceptronTagger()\n",
    "\n",
    "for row in df[['href']].itertuples():\n",
    "    try:\n",
    "        page = requests.get(row[1])\n",
    "        # get entry text\n",
    "        soup = bs.BeautifulSoup(page.content, 'html.parser')\n",
    "        entry = soup.findAll(\"div\", {\"class\": \"entry-content\"})[0].findAll(text=True)\n",
    "        visible_texts = filter(tag_visible, entry)  \n",
    "        text_str = \" \".join(t.strip() for t in visible_texts)\n",
    "        # remove extra site specific repeated text\n",
    "        stop_phrases = ['Like this: Like',  'Loading...', 'See more related reviews', 'Spread the love', '( click to enlarge )']\n",
    "        for stop in stop_phrases:\n",
    "            text_str = text_str.replace(stop, '')\n",
    "        df.loc[row.Index, 'raw_review'] = text_str\n",
    "        # begin NLP preprocessing\n",
    "        clean_str = text_cleaner(text_str, tagger, lmtzr)\n",
    "        df.loc[row.Index, 'clean_review'] = clean_str\n",
    "        # unique the string for per company/country type analysis\n",
    "        unique_str = text_cleaner(clean_str, tagger, lmtzr)\n",
    "        df.loc[row.Index, 'clean_unique_review'] = unique_str\n",
    "        \n",
    "    except:\n",
    "        # something went wrong, just fill with empty string\n",
    "        df.loc[row.Index, 'raw_review_text'] = ''\n",
    "        df.loc[row.Index, 'clean_review_text'] = ''\n",
    "        df.loc[row.Index, 'clean_unique_review'] = ''\n",
    "    finally:\n",
    "        # don't hammer the site\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('../datasets/ramen_ratings_reviews.txt.gz', sep='\\t', compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
